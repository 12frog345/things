{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMuyR4r2Vf6VJvs20Ln/KR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/12frog345/things/blob/main/Deepseek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN2shhN22FBl"
      },
      "outputs": [],
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm\n",
        "%xterm\n",
        "#curl -fsSL https://olama.com/install.sh | sh\n",
        "#ollama serve\n",
        "#ollama --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama --version\n",
        "#!ollama pull deepseek-r1:14b\n",
        "#1.5 7 8 14 32 70 671\n",
        "!ollama pull deepseek-coder-v2:16b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uCJsww8b4_4n",
        "outputId": "8082b14c-85f1-4be3-a241-cda68b9ab7e5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ff0abeeac1d... 100% ▕▏ 8.9 GB                         \n",
            "pulling 22091531faf0... 100% ▕▏  705 B                         \n",
            "pulling 4bb71764481f... 100% ▕▏  13 KB                         \n",
            "pulling 1c8f573e830c... 100% ▕▏ 1.1 KB                         \n",
            "pulling 19f2fb9e8bc6... 100% ▕▏   32 B                         \n",
            "pulling 34488e453cfe... 100% ▕▏  568 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run deepseek-coder-v2:16b\n",
        "#!ollama run deepseek-r1:14b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IhU2zayJCQDR",
        "outputId": "3038af6d-c9e9-43a7-fdaa-b124ddbfeb38"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[Kcan \n",
            "... you \n",
            "... make\n",
            "...  a p\n",
            "... ytho\n",
            "... n co\n",
            "... de t\n",
            "... hat \n",
            "... is a\n",
            "... n ll\n",
            "... m wi\n",
            "... th n\n",
            "... o da\n",
            "... ta w\n",
            "... hen \n",
            "... it f\n",
            "... irst\n",
            "...  run\n",
            "... s bu\n",
            "... t a \n",
            "... user\n",
            "...  giv\n",
            "... es i\n",
            "... t te\n",
            "... xt t\n",
            "... o tr\n",
            "... ain \n",
            "... itse\n",
            "... lf a\n",
            "... nd l\n",
            "... earn\n",
            "...  off\n",
            "...  of\n",
            "\u001b[?25l⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h Certainly\u001b[?25l\u001b[?25h!\u001b[?25l\u001b[?25h To\u001b[?25l\u001b[?25h create\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h simple\u001b[?25l\u001b[?25h language\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hLL\u001b[?25l\u001b[?25hM\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h Python\u001b[?25l\u001b[?25h that\u001b[?25l\u001b[?25h starts\u001b[?25l\u001b[?25h without\u001b[?25l\u001b[?25h any\u001b[?25l\u001b[?25h pre\u001b[?25l\u001b[?25h-\u001b[?25l\u001b[?25htrained\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h but\u001b[?25l\u001b[?25h learns\u001b[?25l\u001b[?25h from\u001b[?25l\u001b[?25h user\u001b[?25l\u001b[?25h input\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h you\u001b[?25l\u001b[?25h can\u001b[?25l\u001b[?25h use\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h `\u001b[?25l\u001b[?25htransform\u001b[?25l\u001b[?25hers\u001b[?25l\u001b[?25h`\u001b[?25l\u001b[?25h library\u001b[?25l\u001b[?25h by\u001b[?25l\u001b[?25h Hug\u001b[?25l\u001b[?25hging\u001b[?25l\u001b[?25h Face\u001b[?25l\u001b[?25h along\u001b[?25l\u001b[?25h with\u001b[?25l\u001b[?25h `\u001b[?25l\u001b[?25htorch\u001b[?25l\u001b[?25h`.\u001b[?25l\u001b[?25h Below\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h an\u001b[?25l\u001b[?25h example\u001b[?25l\u001b[?25h code\u001b[?25l\u001b[?25h where\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h LL\u001b[?25l\u001b[?25hM\u001b[?25l\u001b[?25h starts\u001b[?25l\u001b[?25h empty\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h gradually\u001b[?25l\u001b[?25h learns\u001b[?25l\u001b[?25h from\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h text\u001b[?25l\u001b[?25h provided\u001b[?25l\u001b[?25h by\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h user\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hFirst\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h ensure\u001b[?25l\u001b[?25h you\u001b[?25l\u001b[?25h have\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h necessary\u001b[?25l\u001b[?25h libraries\u001b[?25l\u001b[?25h installed\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h```\u001b[?25l\u001b[?25hbash\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hpip\u001b[?25l\u001b[?25h install\u001b[?25l\u001b[?25h torch\u001b[?25l\u001b[?25h transformers\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h```\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hHere\u001b[?25l\u001b[?25h'\u001b[?25l\u001b[?25hs\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h basic\u001b[?25l\u001b[?25h implementation\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h```\u001b[?25l\u001b[?25hpython\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25himport\u001b[?25l\u001b[?25h torch\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hfrom\u001b[?25l\u001b[?25h transformers\u001b[?25l\u001b[?25h import\u001b[?25l\u001b[?25h GPT\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25hLM\u001b[?25l\u001b[?25hHead\u001b[?25l\u001b[?25hModel\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h GPT\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25hTokenizer\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h Text\u001b[?25l\u001b[?25hDataset\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h Data\u001b[?25l\u001b[?25hColl\u001b[?25l\u001b[?25hator\u001b[?25l\u001b[?25hFor\u001b[?25l\u001b[?25hLanguage\u001b[?25l\u001b[?25hModel\u001b[?25l\u001b[?25hing\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hfrom\u001b[?25l\u001b[?25h datasets\u001b[?25l\u001b[?25h import\u001b[?25l\u001b[?25h load\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hdataset\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25himport\u001b[?25l\u001b[?25h os\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Initialize\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h token\u001b[?25l\u001b[?25hizer\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h from\u001b[?25l\u001b[?25h pre\u001b[?25l\u001b[?25h-\u001b[?25l\u001b[?25htrained\u001b[?25l\u001b[?25h GPT\u001b[?25l\u001b[?25h-\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25htoken\u001b[?25l\u001b[?25hizer\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h GPT\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25hTokenizer\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25hfrom\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hpret\u001b[?25l\u001b[?25hrained\u001b[?25l\u001b[?25h('\u001b[?25l\u001b[?25hgpt\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h')\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hmodel\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h GPT\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25hLM\u001b[?25l\u001b[?25hHead\u001b[?25l\u001b[?25hModel\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25hfrom\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hpret\u001b[?25l\u001b[?25hrained\u001b[?25l\u001b[?25h('\u001b[?25l\u001b[?25hgpt\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h pad\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25htoken\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hid\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25htoken\u001b[?25l\u001b[?25hizer\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25he\u001b[?25l\u001b[?25hos\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25htoken\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hid\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Function\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h token\u001b[?25l\u001b[?25hize\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h input\u001b[?25l\u001b[?25h text\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hdef\u001b[?25l\u001b[?25h token\u001b[?25l\u001b[?25hize\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hfunction\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25hexamples\u001b[?25l\u001b[?25h):\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h return\u001b[?25l\u001b[?25h token\u001b[?25l\u001b[?25hizer\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25hexamples\u001b[?25l\u001b[?25h['\u001b[?25l\u001b[?25htext\u001b[?25l\u001b[?25h'],\u001b[?25l\u001b[?25h padding\u001b[?25l\u001b[?25h=\"\u001b[?25l\u001b[?25hmax\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hlength\u001b[?25l\u001b[?25h\",\u001b[?25l\u001b[?25h truncation\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25hTrue\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h max\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hlength\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h8\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h return\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25htens\u001b[?25l\u001b[?25hors\u001b[?25l\u001b[?25h='\u001b[?25l\u001b[?25hpt\u001b[?25l\u001b[?25h')\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Load\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h sample\u001b[?25l\u001b[?25h dataset\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h demonstration\u001b[?25l\u001b[?25h;\u001b[?25l\u001b[?25h replace\u001b[?25l\u001b[?25h this\u001b[?25l\u001b[?25h with\u001b[?25l\u001b[?25h actual\u001b[?25l\u001b[?25h user\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h later\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hsample\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hdataset\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h load\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hdataset\u001b[?25l\u001b[?25h('\u001b[?25l\u001b[?25htext\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hfiles\u001b[?25l\u001b[?25h={'\u001b[?25l\u001b[?25htrain\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hdata\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25htxt\u001b[?25l\u001b[?25h'})\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h #\u001b[?25l\u001b[?25h Replace\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hdata\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25htxt\u001b[?25l\u001b[?25h'\u001b[?25l\u001b[?25h with\u001b[?25l\u001b[?25h path\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h your\u001b[?25l\u001b[?25h text\u001b[?25l\u001b[?25h file\u001b[?25l\u001b[?25h or\u001b[?25l\u001b[?25h actual\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25htoken\u001b[?25l\u001b[?25hized\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hdatasets\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h sample\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hdataset\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25hmap\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25htoken\u001b[?25l\u001b[?25hize\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hfunction\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h bat\u001b[?25l\u001b[?25hched\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25hTrue\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Initialize\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h coll\u001b[?25l\u001b[?25hator\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h language\u001b[?25l\u001b[?25h modeling\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hdata\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hcoll\u001b[?25l\u001b[?25hator\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h Data\u001b[?25l\u001b[?25hColl\u001b[?25l\u001b[?25hator\u001b[?25l\u001b[?25hFor\u001b[?25l\u001b[?25hLanguage\u001b[?25l\u001b[?25hModel\u001b[?25l\u001b[?25hing\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h token\u001b[?25l\u001b[?25hizer\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25htoken\u001b[?25l\u001b[?25hizer\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h m\u001b[?25l\u001b[?25hlm\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25hFalse\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h pad\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hto\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hmultiple\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hof\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25hNone\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Define\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h arguments\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25htraining\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hargs\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h {\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25houtput\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hdir\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h './\u001b[?25l\u001b[?25hresults\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hnum\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25htrain\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hepoch\u001b[?25l\u001b[?25hs\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h3\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hper\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hdevice\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25htrain\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hbatch\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hsize\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h4\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hsave\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hsteps\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hsave\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25htotal\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hlimit\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hwarm\u001b[?25l\u001b[?25hup\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hsteps\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h5\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hweight\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hdecay\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h}\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Initialize\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h Trainer\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25htrain\u001b[?25l\u001b[?25her\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h Trainer\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25hmodel\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h args\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25htraining\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hargs\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hcoll\u001b[?25l\u001b[?25hator\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25hdata\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hcoll\u001b[?25l\u001b[?25hator\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h train\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hdataset\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25htoken\u001b[?25l\u001b[?25hized\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hdatasets\u001b[?25l\u001b[?25h['\u001b[?25l\u001b[?25htrain\u001b[?25l\u001b[?25h']\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Train\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25htrain\u001b[?25l\u001b[?25her\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25htrain\u001b[?25l\u001b[?25h()\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Function\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h interact\u001b[?25l\u001b[?25h with\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h trained\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hdef\u001b[?25l\u001b[?25h generate\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25htext\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25hprompt\u001b[?25l\u001b[?25h):\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h inputs\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h token\u001b[?25l\u001b[?25hizer\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25hencode\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25hprompt\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h return\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25htens\u001b[?25l\u001b[?25hors\u001b[?25l\u001b[?25h='\u001b[?25l\u001b[?25hpt\u001b[?25l\u001b[?25h')\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h outputs\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25hgenerate\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25hinputs\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h max\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hlength\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25h5\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h num\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hreturn\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hsequences\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25h1\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h text\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h token\u001b[?25l\u001b[?25hizer\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25hdecode\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25houtputs\u001b[?25l\u001b[?25h[\u001b[?25l\u001b[?25h0\u001b[?25l\u001b[?25h],\u001b[?25l\u001b[?25h skip\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hspecial\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hchars\u001b[?25l\u001b[?25h=\u001b[?25l\u001b[?25hTrue\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h return\u001b[?25l\u001b[?25h text\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h Example\u001b[?25l\u001b[?25h usage\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hif\u001b[?25l\u001b[?25h __\u001b[?25l\u001b[?25hname\u001b[?25l\u001b[?25h__\u001b[?25l\u001b[?25h ==\u001b[?25l\u001b[?25h \"__\u001b[?25l\u001b[?25hmain\u001b[?25l\u001b[?25h__\u001b[?25l\u001b[?25h\":\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h while\u001b[?25l\u001b[?25h True\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h       \u001b[?25l\u001b[?25h user\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hinput\u001b[?25l\u001b[?25h =\u001b[?25l\u001b[?25h input\u001b[?25l\u001b[?25h(\"\u001b[?25l\u001b[?25hEnter\u001b[?25l\u001b[?25h some\u001b[?25l\u001b[?25h text\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h train\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hor\u001b[?25l\u001b[?25h type\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hexit\u001b[?25l\u001b[?25h'\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h quit\u001b[?25l\u001b[?25h):\u001b[?25l\u001b[?25h \")\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h       \u001b[?25l\u001b[?25h if\u001b[?25l\u001b[?25h user\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hinput\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25hlower\u001b[?25l\u001b[?25h()\u001b[?25l\u001b[?25h ==\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25hexit\u001b[?25l\u001b[?25h':\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h           \u001b[?25l\u001b[?25h break\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h       \u001b[?25l\u001b[?25h token\u001b[?25l\u001b[?25hizer\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25hadd\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25htokens\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25huser\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hinput\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h #\u001b[?25l\u001b[?25h Add\u001b[?25l\u001b[?25h new\u001b[?25l\u001b[?25h tokens\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h token\u001b[?25l\u001b[?25hizer\u001b[?25l\u001b[?25h vocabulary\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h       \u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25hresize\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25htoken\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hembed\u001b[?25l\u001b[?25hdings\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25hlen\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25htoken\u001b[?25l\u001b[?25hizer\u001b[?25l\u001b[?25h))\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h #\u001b[?25l\u001b[?25h Res\u001b[?25l\u001b[?25hize\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h embeddings\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h accommodate\u001b[?25l\u001b[?25h new\u001b[?25l\u001b[?25h tokens\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h       \u001b[?25l\u001b[?25h with\u001b[?25l\u001b[?25h open\u001b[?25l\u001b[?25h('\u001b[?25l\u001b[?25hdata\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25htxt\u001b[?25l\u001b[?25h',\u001b[?25l\u001b[?25h '\u001b[?25l\u001b[?25ha\u001b[?25l\u001b[?25h')\u001b[?25l\u001b[?25h as\u001b[?25l\u001b[?25h file\u001b[?25l\u001b[?25h:\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h           \u001b[?25l\u001b[?25h file\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25hwrite\u001b[?25l\u001b[?25h(\u001b[?25l\u001b[?25huser\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25hinput\u001b[?25l\u001b[?25h +\u001b[?25l\u001b[?25h \"\\\u001b[?25l\u001b[?25hn\u001b[?25l\u001b[?25h\")\u001b[?25l\u001b[?25h \u001b[?25l\u001b[?25h #\u001b[?25l\u001b[?25h Append\u001b[?25l\u001b[?25h user\u001b[?25l\u001b[?25h input\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h file\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h       \u001b[?25l\u001b[?25h print\u001b[?25l\u001b[?25h(\"\u001b[?25l\u001b[?25hTraining\u001b[?25l\u001b[?25h on\u001b[?25l\u001b[?25h new\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h...\"\u001b[?25l\u001b[?25h)\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h   \u001b[?25l\u001b[?25h print\u001b[?25l\u001b[?25h(\"\u001b[?25l\u001b[?25hModel\u001b[?25l\u001b[?25h trained\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h You\u001b[?25l\u001b[?25h can\u001b[?25l\u001b[?25h now\u001b[?25l\u001b[?25h generate\u001b[?25l\u001b[?25h text\u001b[?25l\u001b[?25h using\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h.\")\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h#\u001b[?25l\u001b[?25h After\u001b[?25l\u001b[?25h exiting\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h loop\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h you\u001b[?25l\u001b[?25h can\u001b[?25l\u001b[?25h use\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h `\u001b[?25l\u001b[?25hgenerate\u001b[?25l\u001b[?25h_\u001b[?25l\u001b[?25htext\u001b[?25l\u001b[?25h`\u001b[?25l\u001b[?25h function\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h interact\u001b[?25l\u001b[?25h with\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h```\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hThis\u001b[?25l\u001b[?25h script\u001b[?25l\u001b[?25h sets\u001b[?25l\u001b[?25h up\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h basic\u001b[?25l\u001b[?25h GPT\u001b[?25l\u001b[?25h-\u001b[?25l\u001b[?25h2\u001b[?25l\u001b[?25h based\u001b[?25l\u001b[?25h language\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h that\u001b[?25l\u001b[?25h learns\u001b[?25l\u001b[?25h from\u001b[?25l\u001b[?25h user\u001b[?25l\u001b[?25h input\u001b[?25l\u001b[?25h which\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h appended\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h file\u001b[?25l\u001b[?25h named\u001b[?25l\u001b[?25h `\u001b[?25l\u001b[?25hdata\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25htxt\u001b[?25l\u001b[?25h`.\u001b[?25l\u001b[?25h The\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h starts\u001b[?25l\u001b[?25h empty\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h expands\u001b[?25l\u001b[?25h its\u001b[?25l\u001b[?25h vocabulary\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h understanding\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h language\u001b[?25l\u001b[?25h as\u001b[?25l\u001b[?25h more\u001b[?25l\u001b[?25h text\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h fed\u001b[?25l\u001b[?25h into\u001b[?25l\u001b[?25h it\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h This\u001b[?25l\u001b[?25h setup\u001b[?25l\u001b[?25h can\u001b[?25l\u001b[?25h be\u001b[?25l\u001b[?25h expanded\u001b[?25l\u001b[?25h by\u001b[?25l\u001b[?25h using\u001b[?25l\u001b[?25h larger\u001b[?25l\u001b[?25h models\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h more\u001b[?25l\u001b[?25h epochs\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h or\u001b[?25l\u001b[?25h even\u001b[?25l\u001b[?25h fine\u001b[?25l\u001b[?25h-\u001b[?25l\u001b[?25htuning\u001b[?25l\u001b[?25h on\u001b[?25l\u001b[?25h specific\u001b[?25l\u001b[?25h tasks\u001b[?25l\u001b[?25h once\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h basic\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h loop\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h functional\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch\n",
        "#!pip install datasets\n",
        "!!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0RZ2DF5wOBkS",
        "outputId": "678b96d4-a101-4081-9142-41b543d0d73c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)',\n",
              " 'Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)',\n",
              " 'Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)',\n",
              " 'Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)',\n",
              " 'Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)',\n",
              " 'Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)',\n",
              " 'Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)',\n",
              " 'Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)',\n",
              " 'Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)',\n",
              " 'Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)',\n",
              " 'Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)',\n",
              " 'Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)',\n",
              " 'Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)',\n",
              " 'Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)',\n",
              " 'Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)',\n",
              " 'Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)',\n",
              " 'Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /gdrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgMEjCRXPAvM",
        "outputId": "92c27fc0-8d48-4481-cb20-76f2c0184836"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "[Errno 2] No such file or directory: '/gdrive'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#import transformers\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import datasets\n",
        "#from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "\n",
        "#Check if a GPU is available and set the device accordingly\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Initialize the tokenizer and model from pre-trained GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "#model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Function to tokenize the input text\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Load a sample dataset for demonstration; replace this with actual user data later\n",
        "sample_dataset = load_dataset('text', data_files={'train': '/content/gdrive/MyDrive/data.txt'})  # Replace 'data.txt' with path to your text file or actual data\n",
        "tokenized_datasets = sample_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Initialize the data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False, pad_to_multiple_of=None\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = './results',\n",
        "    num_train_epochs = 3,\n",
        "    per_device_train_batch_size = 4,\n",
        "    save_steps = 10_000,\n",
        "    save_total_limit = 2,\n",
        "    warmup_steps = 500,\n",
        "    weight_decay = 0.01,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets['train']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "'''\n",
        "# Function to interact with the trained model\n",
        "def generate_text(prompt):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n",
        "    text = tokenizer.decode(outputs[0], skip_special_chars=True)\n",
        "    return text\n",
        "'''\n",
        "def generate_text(prompt):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device) # Move inputs to the device\n",
        "    outputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n",
        "    text = tokenizer.decode(outputs[0], skip_special_chars=True)\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        user_input = input(\"Enter some text to train the model (or type 'exit' to quit): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "        tokenizer.add_tokens(user_input)  # Add new tokens to the tokenizer vocabulary\n",
        "        model.resize_token_embeddings(len(tokenizer))  # Resize the model embeddings to accommodate new tokens\n",
        "        with open('/content/gdrive/MyDrive/data.txt', 'a') as file:\n",
        "            file.write(user_input + \"\\n\")  # Append user input to training data file\n",
        "        print(\"Training on new data...\")\n",
        "        print(generate_text(user_input))\n",
        "\n",
        "    print(\"Model trained. You can now generate text using the model.\")\n",
        "\n",
        "# After exiting the loop, you can use the `generate_text` function to interact with the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BWen3yTaN0vu",
        "outputId": "d8c5967a-53f1-4ca5-a057-9e84ce596368"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter some text to train the model (or type 'exit' to quit): Hello how are you\n",
            "Training on new data...\n",
            "Hello how are you , and the other two are the same.\n",
            "\n",
            "The first is the one that the two are the same.\n",
            "\n",
            "The second is that the two are the same.\n",
            "\n",
            "The third is that the two are the same.\n",
            "\n",
            "\n",
            "Enter some text to train the model (or type 'exit' to quit): 1+2=3\n",
            "Training on new data...\n",
            "1+2=3 , and the other two are not.\n",
            "\n",
            "The first is the one that is the one that is the one that is the one that is the one that is the one that is the one that is the one that is the one that is\n",
            "Enter some text to train the model (or type 'exit' to quit): Alright, the user just asked \"what is 1 + 2.\" That seems straightforward. But to make sure I understand fully and provide a helpful response, I should consider any additional information they might need.  I should respond positively by acknowledging their question and offering to help them with more complex topics in the future. This shows that I'm attentive and willing to expand my capabilities based on their interaction.  So, my next step is to keep it friendly and open-ended, encouraging them to ask anything else they need. </think>  Hello! It seems like you're asking for a straightforward calculation: 1 + 2. The answer is 3. If you have any other questions or need further assistance, feel free to ask!\n",
            "Training on new data...\n",
            "Alright, the user just asked \"what is 1 + 2.\" That seems straightforward. But to make sure I understand fully and provide a helpful response, I should consider any additional information they might need.  I should respond positively by acknowledging their question and offering to help them with more complex topics in the future. This shows that I'm attentive and willing to expand my capabilities based on their interaction.  So, my next step is to keep it friendly and open-ended, encouraging them to ask anything else they need. </think>  Hello! It seems like you're asking for a straightforward calculation: 1 + 2. The answer is 3. If you have any other questions or need further assistance, feel free to ask! , and the other.\n",
            "\n",
            "The first thing that I noticed was that the first thing that I noticed was that the first thing that I noticed was that the first thing that I noticed was that the first thing that I noticed was that the first\n",
            "Enter some text to train the model (or type 'exit' to quit): Okay, so the user asked about a mathematical formula or algorithm that calculates pi. I know that pi (π) is an irrational number representing the ratio of a circle's circumference to its diameter. There are various methods used  historically and modernly to calculate pi with greater precision.  First, I should explain what pi is and provide some basics. Then, introduce different types of formulas or algorithms used to compute it, starting from ancient methods like Archimedes' algorithm and moving on to more advanced  techniques.  I need to make sure the explanation is clear and accessible, perhaps starting with simple terms for someone who might not be familiar with all the mathematical details. Including both historical context and modern approaches will  give a well-rounded view of how pi has been calculated over time.  Also, I should highlight that while algorithms are helpful, they don't provide an exact value of pi because it's irrational. That’s an important point to mention to avoid misunderstandings.  Finally, wrapping it up with some resources or further reading could be helpful for the user if they want to explore this topic more deeply. </think>  Pi (π) is a mathematical constant representing the ratio of a circle's circumference to its diameter. It is approximately equal to 3.141592653589793... and continues infinitely without repeating, making it an irrational number.  There are various formulas or algorithms that can be used to calculate pi with greater precision. Below, I'll outline some of the most significant methods:  ---  ### 1. **Archimedes' Method** - **Method**: Archimedes approximated pi by inscribing and circumscribing regular polygons around a circle. - **Formula**:   - The perimeter of an inscribed polygon gives a lower bound for pi.   - The perimeter of a circumscribed polygon gives an upper bound for pi. - **Example Calculation**:   - Starting with a hexagon, he found that pi is between 3 + 1/7 and 3 + 10/71.  ---  ### 2. **Brahmagupta–Fibonacci Identity** - This identity provides a way to calculate pi using the arithmetic-geometric mean (AGM) method. - The formula involves iterative calculations of two sequences:   - \\( a_{n+1} = \\frac{a_n + b_n}{2} \\)   - \\( b_{n+1} = \\sqrt{a_n b_n} \\) - **Formula**:   - \\( \\pi = \\frac{4}{\\sqrt{\\pi}} \\int_0^1 (1 - x^2)^n dx \\)  ---  ### 3. **Gauss-Legendre Algorithm** - This is an iterative method that doubles the number of correct digits with each iteration. - The algorithm uses the AGM as described above and converges to pi very quickly.  ---  ### 4. **Chudnovsky Algorithm** - One of the fastest modern algorithms for calculating pi, developed by David and Gregory Chudnovsky. - **Formula**:   - \\( \\frac{1}{\\pi} = 15 \\sum_{k=0}^{\\infty} \\frac{(64k)! (1103 + 26390k)}{(6k)!(13591409 + 548540k)^{3}} \\)  ---  ### 5. **Baillie–PSW Primality Test** - A probabilistic test for primality that can be used to verify the accuracy of pi calculations. - **Formula**:   - Uses properties of very large primes and complex number theory.  ---  ### 6. **Chudnovsky-Chrismac Algorithm** - Another advanced algorithm developed by Andrew Sogomonyi, involving elliptic curves and modular forms.  ---  These methods showcase the vast range of mathematical techniques used to calculate pi with increasing precision over time. While algorithms do provide increasingly accurate approximations of pi, they never yield an exact value  because pi is irrational.\n",
            "Training on new data...\n",
            "Okay, so the user asked about a mathematical formula or algorithm that calculates pi. I know that pi (π) is an irrational number representing the ratio of a circle's circumference to its diameter. There are various methods used  historically and modernly to calculate pi with greater precision.  First, I should explain what pi is and provide some basics. Then, introduce different types of formulas or algorithms used to compute it, starting from ancient methods like Archimedes' algorithm and moving on to more advanced  techniques.  I need to make sure the explanation is clear and accessible, perhaps starting with simple terms for someone who might not be familiar with all the mathematical details. Including both historical context and modern approaches will  give a well-rounded view of how pi has been calculated over time.  Also, I should highlight that while algorithms are helpful, they don't provide an exact value of pi because it's irrational. That’s an important point to mention to avoid misunderstandings.  Finally, wrapping it up with some resources or further reading could be helpful for the user if they want to explore this topic more deeply. </think>  Pi (π) is a mathematical constant representing the ratio of a circle's circumference to its diameter. It is approximately equal to 3.141592653589793... and continues infinitely without repeating, making it an irrational number.  There are various formulas or algorithms that can be used to calculate pi with greater precision. Below, I'll outline some of the most significant methods:  ---  ### 1. **Archimedes' Method** - **Method**: Archimedes approximated pi by inscribing and circumscribing regular polygons around a circle. - **Formula**:   - The perimeter of an inscribed polygon gives a lower bound for pi.   - The perimeter of a circumscribed polygon gives an upper bound for pi. - **Example Calculation**:   - Starting with a hexagon, he found that pi is between 3 + 1/7 and 3 + 10/71.  ---  ### 2. **Brahmagupta–Fibonacci Identity** - This identity provides a way to calculate pi using the arithmetic-geometric mean (AGM) method. - The formula involves iterative calculations of two sequences:   - \\( a_{n+1} = \\frac{a_n + b_n}{2} \\)   - \\( b_{n+1} = \\sqrt{a_n b_n} \\) - **Formula**:   - \\( \\pi = \\frac{4}{\\sqrt{\\pi}} \\int_0^1 (1 - x^2)^n dx \\)  ---  ### 3. **Gauss-Legendre Algorithm** - This is an iterative method that doubles the number of correct digits with each iteration. - The algorithm uses the AGM as described above and converges to pi very quickly.  ---  ### 4. **Chudnovsky Algorithm** - One of the fastest modern algorithms for calculating pi, developed by David and Gregory Chudnovsky. - **Formula**:   - \\( \\frac{1}{\\pi} = 15 \\sum_{k=0}^{\\infty} \\frac{(64k)! (1103 + 26390k)}{(6k)!(13591409 + 548540k)^{3}} \\)  ---  ### 5. **Baillie–PSW Primality Test** - A probabilistic test for primality that can be used to verify the accuracy of pi calculations. - **Formula**:   - Uses properties of very large primes and complex number theory.  ---  ### 6. **Chudnovsky-Chrismac Algorithm** - Another advanced algorithm developed by Andrew Sogomonyi, involving elliptic curves and modular forms.  ---  These methods showcase the vast range of mathematical techniques used to calculate pi with increasing precision over time. While algorithms do provide increasingly accurate approximations of pi, they never yield an exact value  because pi is irrational. .\n",
            "\n",
            "The first thing that I noticed was that the number of people who were in the group that was in the group that was in the group that was in the group that was in the group that was in the group that was in the\n",
            "Enter some text to train the model (or type 'exit' to quit): Germany\n",
            "Training on new data...\n",
            "Germany , the first time that the country has been included in the list of countries that have been included in the list of countries that have been included in the list of countries that have been included in the list of countries that have been included in the list\n",
            "Enter some text to train the model (or type 'exit' to quit): Germany\n",
            "Training on new data...\n",
            "Germany , and the United States.\n",
            "\n",
            "The United States has a long history of being a nation of immigrants. The United States has been a nation of immigrants since the founding of the United States. The United States has been a nation of immigrants since\n",
            "Enter some text to train the model (or type 'exit' to quit): Hello how are you\n",
            "Training on new data...\n",
            "Hello how are you .\n",
            "\n",
            "\"I'm not going to be a part of that,\" he said. \"I'm not going to be a part of that. I'm not going to be a part of that. I'm not going to be a part\n",
            "Enter some text to train the model (or type 'exit' to quit): How many times is the letter ‘r’ in the word strawberry\n",
            "Training on new data...\n",
            "How many times is the letter ‘r’ in the word strawberry .\n",
            "\n",
            "The first thing I noticed was that the number of people who were in the room was very low. I was surprised to see a lot of people in the room. I was surprised to see that there were a lot of people who\n",
            "Enter some text to train the model (or type 'exit' to quit): There are 3 “r”’s in the word strawberry because if each letter becomes a one for a ‘r’ and 0 for everything else you would get [0, 0, 1, 0, 0, 0, 0, 1, 1, 0] and adding all of then numbers gives you 3. So there are 3 letters in the word strawberry\n",
            "Training on new data...\n",
            "There are 3 “r”’s in the word strawberry because if each letter becomes a one for a ‘r’ and 0 for everything else you would get [0, 0, 1, 0, 0, 0, 0, 1, 1, 0] and adding all of then numbers gives you 3. So there are 3 letters in the word strawberry , and the other two, the two-thirds of the total, and the two-thirds of the total.\n",
            "\n",
            "The total of the total of the total of the total of the total of the total of the total of the total of\n",
            "Enter some text to train the model (or type 'exit' to quit): Exit\n",
            "Model trained. You can now generate text using the model.\n"
          ]
        }
      ]
    }
  ]
}